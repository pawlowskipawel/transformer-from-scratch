# AUTOGENERATED! DO NOT EDIT! File to edit: nbs/data.ipynb (unless otherwise specified).

__all__ = ['TranslationDataset']

# Cell
from torch.utils.data import Dataset

# Cell
class TranslationDataset(Dataset):
    def __init__(self, src_sentences, trg_sentences, src_tokenizer, trg_tokenizer, lowercase=True, max_len=50):
        self._src_sentences = src_sentences
        self._trg_sentences = trg_sentences

        self._src_tokenizer = src_tokenizer
        self._trg_tokenizer = trg_tokenizer

        self._lowercase = lowercase
        self._max_len = max_len

    def __len__(self):
        return len(self._src_sentences)

    def tokenize_text(self, tokenizer, text, add_special_tokens):
        return tokenizer(text,
                         padding=True,
                         truncation=True,
                         return_tensors=True,
                         max_len=self._max_len,
                         add_special_tokens=add_special_tokens)

    def __getitem__(self, index):
        src_sentence = self._src_sentences[index]
        trg_sentence = self._trg_sentences[index]

        if self._lowercase:
            src_sentence = src_sentence.lower()
            trg_sentence = trg_sentence.lower()

        src_tokenized = self.tokenize_text(self._src_tokenizer, src_sentence, add_special_tokens=True)
        trg_tokenized = self.tokenize_text(self._trg_tokenizer, trg_sentence, add_special_tokens=True)

        return {
            "src_sentence": {
                "text": src_sentence,
                "input_ids": src_tokenized["input_ids"],
                "padding_mask": src_tokenized["padding_mask"],
            },
            "trg_sentence": {
                "text": trg_sentence,
                "input_ids": trg_tokenized["input_ids"],
                "padding_mask": trg_tokenized["padding_mask"],
            }
        }