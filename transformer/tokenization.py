# AUTOGENERATED! DO NOT EDIT! File to edit: nbs/tokenization.ipynb (unless otherwise specified).

__all__ = ['Tokenizer']

# Cell
from torchtext.data.utils import get_tokenizer

import numpy as np
import torch
import re

# Cell
class Tokenizer:

    # custom word-level tokenizer

    def __init__(self, language, vocabulary=None):

        assert language in ("en", "de"), f"Unknown language: {language}. Supported languages: en, de"
        language = "en_core_web_sm" if language == "en" else "de_core_news_sm"

        self._tokenizer = get_tokenizer("spacy", language=language)

        self._sos_token = "<sos>"
        self._eos_token = "<eos>"
        self._pad_token = "<pad>"

        if vocabulary is not None:
            self._vocab = torch.load(vocabulary) if isinstance(vocabulary, str) else vocabulary
            self._pad_token_id = self._vocab[self._pad_token]
            self._sos_token_id = self._vocab[self._sos_token]
            self._eos_token_id = self._vocab[self._eos_token]


    @property
    def pad_token_id(self):
        return self._pad_token_id

    @property
    def sos_token_id(self):
        return self._sos_token_id

    @property
    def eos_token_id(self):
        return self._eos_token_id

    @property
    def sos_token(self):
        return self._sos_token

    @property
    def eos_token(self):
        return self._eos_token

    def get_vocab_len(self):
        return len(self._vocab)

    def _truncation(self, input_ids, max_len, add_special_tokens):
        return input_ids[list(range(max_len-1)) + [-1]] if add_special_tokens else input_ids[:max_len]

    def _padding(self, input_ids, max_len):
        padding_tensor = np.full((max_len - len(input_ids),), self._pad_token_id)

        return np.concatenate((input_ids, padding_tensor), axis=0)

    def _get_padding_mask(self, input_ids):
        return np.where(input_ids == self._pad_token_id, 0, 1)

    def __call__(self, text, return_tensors=True, padding=False, truncation=False, max_len=None, add_special_tokens=False):
        tokens = self._tokenizer(re.sub(r" +", " ", text))
        ids = np.array(self._vocab(tokens))

        if add_special_tokens:
            ids = np.hstack([[self._sos_token_id], ids, [self._eos_token_id]])

        if max_len is not None:
            if padding and len(ids) < max_len:
                ids = self._padding(ids, max_len)
            if truncation and len(ids) > max_len:
                ids = self._truncation(ids, max_len, add_special_tokens)


        padding_mask = self._get_padding_mask(ids)

        return {
            "tokens": tokens,
            "input_ids": torch.tensor(ids, dtype=torch.long) if return_tensors else ids,
            "padding_mask": torch.tensor(padding_mask, dtype=torch.long) if return_tensors else padding_mask
        }

    def encode(self, text):
        return self._vocab.lookup_indices(self(text))

    def decode(self, ids):
        if not isinstance(ids, list):
            ids = ids.tolist()

        return self._vocab.lookup_tokens(ids)
